{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526c2d73-6739-4eee-a844-e749f8c5510c",
   "metadata": {},
   "source": [
    "# Mapping\n",
    "\n",
    "Try a RAG approach of mapping incoming message formats to the common data format.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4b283-e683-43cd-82a1-709b08304f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc867fcf-0e53-4563-b6b7-5aab953576d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet \\\n",
    "    langchain==0.0.309 \\\n",
    "    \"transformers>=4.24,<5\" \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    apache-beam \\\n",
    "    datasets \\\n",
    "    tiktoken \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2ac56c-5097-4c04-994a-b5c57d74d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b487cd-a047-446b-ab2d-02a2ac137a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    #assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ed252-e773-41ff-9489-1fcce7bb0966",
   "metadata": {},
   "source": [
    "## Configure langchain\n",
    "\n",
    "We begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude for text generation and Amazon Titan for text embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517579be-07a3-43d3-a476-ccbd8ffbbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2:1\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':5000})\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b227166-2145-4a14-b381-3b1e8db859e4",
   "metadata": {},
   "source": [
    "We need to add the embeddings of our known mappings to the Vector store. The Claude 2.1 FM has a large 200k token input limit therefore we don't need to worry about splitting the templates into smaller chunks to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ea8b83-7fa2-4f6c-bfd2-0b015b39f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:02<00:02,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 5 documents loaded is 1412 characters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('./mappings', glob=\"**/*.md\", show_progress=True)\n",
    "docs = loader.load()\n",
    "\n",
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "avg_char_count = avg_doc_length(docs)\n",
    "print(f'Average length among {len(docs)} documents loaded is {avg_char_count} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6e4cf-5ca0-4eaf-a722-0609ee6a46c8",
   "metadata": {},
   "source": [
    "Sample the embeddings for one of the mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27e397e-1e3e-47f6-a942-4315bd6f2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "# print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "# print(\"Size of the embedding: \", sample_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff776d4-aeda-4c39-92ad-88547b140009",
   "metadata": {},
   "source": [
    "As this is a quick prototype, use FAISS (in-memory vector store) within LangChain. But use OpenSearch Serverless for the hackathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c27ad1-2547-4f63-a363-d0fd4f08bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "def create_vector_store() :\n",
    "    vectorstore_faiss = FAISS.from_documents(\n",
    "        docs,\n",
    "        bedrock_embeddings,\n",
    "    )\n",
    "    # wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)\n",
    "    return vectorstore_faiss\n",
    "\n",
    "vector_store = create_vector_store()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb41df9-8d17-439f-8232-ec6f1314776c",
   "metadata": {},
   "source": [
    "## Obtain the mapped result\n",
    "\n",
    "Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "180f459b-2ea8-406a-afb7-a288fb441d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def execute(prompt, query, expected) :\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(\n",
    "            # search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "            search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5}\n",
    "        ),\n",
    "        # return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    answer = qa({\"query\": query})\n",
    "\n",
    "    if expected != '?':\n",
    "        print(\"Expected: \\n\", expected)\n",
    "        print(\"\\nActual: \\n\", answer['result'])\n",
    "    else:\n",
    "        print(answer['result'])\n",
    "    \n",
    "    # print(\"\\tquery: \\n\", answer['query'])\n",
    "    # print(\"\\tsource_documents: \\n\", answer['source_documents'])\n",
    "\n",
    "def get_relevant_documents(embedding):\n",
    "    relevant_documents = vector_store.similarity_search_by_vector(embedding)\n",
    "    print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "    # print('----')\n",
    "    # for i, rel_doc in enumerate(relevant_documents):\n",
    "    #     print_ww(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    #     print('---')\n",
    "    return relevant_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988d0fb-ebcc-4b33-89c0-44d18cecd3db",
   "metadata": {},
   "source": [
    "Now that we have our vector store in place, we can start asking questions. Let's define a reusable template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f403ca9d-f868-4fc9-8ddc-4c1fa550c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the context within the following <context></context> XML tag to provide a concise answer to the question at the end:\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "<schema>\n",
    "{{\n",
    "  \"$schema\": \"https://json-schema.org/draft/2020-12\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {{\n",
    "    \"sensor_id\": {{\n",
    "      \"type\": \"string\"\n",
    "    }},\n",
    "    \"timestamp\": {{\n",
    "      \"type\": \"string\",\n",
    "      \"format\": \"datetime\"\n",
    "    }},\n",
    "    \"pm0_3\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"pm0_5\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"pm1\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"pm2_5\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"pm4\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"pm5\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"pm10\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"temperature\": {{\n",
    "      \"type\": \"number\"\n",
    "    }},\n",
    "    \"humidity\": {{\n",
    "      \"type\": \"number\"\n",
    "    }}\n",
    "  }},\n",
    "  \"required\": [\n",
    "    \"sensor_id\",\n",
    "    \"timestamp\"\n",
    "  ]\n",
    "}}\n",
    "</schema>\n",
    "\n",
    "[Task instructions]\n",
    "You ALWAYS follow these guidelines when writing your response:\n",
    "<guidelines>\n",
    "- You will be acting as an expert software developer, writing responses as json in the AFRI_SET_COMMON json format. \n",
    "- Return only the converted json as the response, along with a confidence (as a percentage) of how well you did. The json response must adhere to the json schema defined in the <schema></schema> XML tag.\n",
    "- Do not return any other surrounding text, explanation or context.\n",
    "</guidelines>\n",
    "\n",
    "When you reply, first determine how the provided input should be mapped to the AFRI_SET_COMMON json format. Write this mapping within the <thinking></thinking> XML tags. This is a space for you to write down relevant content and will not be shown to the user.  Once you are done extracting determing the mapping steps, answer the question.  Put your answer inside the <response></response> XML tags.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd3e43-de47-44f9-890d-939722c47a16",
   "metadata": {},
   "source": [
    "Define the question(s) we want to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81ccb50c-691d-4881-b769-41bdef2daaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "Expected: \n",
      " [{'sensor_id': 'dc5475b0f97c', 'timestamp': '2023-11-12T01:20:00.000Z', 'pm1': 15.6, 'pm2_5': 26.400002, 'pm10': 27.3, 'temperature': 25.3, 'humidity': 70.8}, {'sensor_id': 'dc5475b0f97d', 'timestamp': '2023-11-12T02:35:00.000Z', 'pm1': 10.65, 'pm2_5': 18.349998, 'pm10': 18.75, 'temperature': 24.6, 'humidity': 73.75}]\n",
      "\n",
      "Actual: \n",
      "  <thinking>\n",
      "To map the provided data to the AFRI_SET_COMMON format:\n",
      "\n",
      "- \"locationId\" maps to \"sensor_id\"\n",
      "- \"timestamp\" already in ISO8601 datetime format, can be used as-is for \"timestamp\"\n",
      "- \"pm01\" maps to \"pm0_3\" \n",
      "- \"pm02\" maps to \"pm0_5\"\n",
      "- \"pm10\" maps to \"pm10\"\n",
      "- \"atmp\" maps to \"temperature\"\n",
      "- \"rhum\" maps to \"humidity\"\n",
      "\n",
      "The other provided fields are not needed to populate the schema.\n",
      "</thinking>\n",
      "\n",
      "<response>\n",
      "[\n",
      "  {\n",
      "    \"sensor_id\": \"59513\",\n",
      "    \"timestamp\": \"2023-11-12T01:20:00.000Z\", \n",
      "    \"pm0_3\": 15.6,\n",
      "    \"pm0_5\": 26.400002,\n",
      "    \"pm10\": 27.3,\n",
      "    \"temperature\": 25.3,\n",
      "    \"humidity\": 70.8\n",
      "  },\n",
      "  {\n",
      "    \"sensor_id\": \"59513\",\n",
      "    \"timestamp\": \"2023-11-12T02:35:00.000Z\",\n",
      "    \"pm0_3\": 10.65,\n",
      "    \"pm0_5\": 18.349998,\n",
      "    \"pm10\": 18.75,\n",
      "    \"temperature\": 24.6,\n",
      "    \"humidity\": 73.75\n",
      "  }\n",
      "]\n",
      "</response>\n",
      "\n",
      "Confidence: 90%\n"
     ]
    }
   ],
   "source": [
    "# This is an example from air-gradient-dc5475b0f97c.csv:\n",
    "\n",
    "query_1 = \"\"\"Map the following provided data to the AFRI_SET_COMMON formmat:\n",
    "locationId,locationName,pm01,pm02,pm10,pm003Count,atmp,rhum,rco2,tvoc,wifi,timestamp,serialno,firmwareVersion,tvocIndex,noxIndex,datapoints\n",
    "59513,dc5475b0f97c,15.6,26.400002,27.3,2994.5,25.3,70.8,,,-69,2023-11-12T01:20:00.000Z,dc5475b0f97c,,,,2\n",
    "59513,dc5475b0f97d,10.65,18.349998,18.75,2125.5,24.6,73.75,,,-68.5,2023-11-12T02:35:00.000Z,dc5475b0f97c,,,,2\n",
    "\"\"\"\n",
    "\n",
    "# this is just used for testing. The model never sees this\n",
    "expected = [{\n",
    "    'sensor_id': \"dc5475b0f97c\",\n",
    "    'timestamp': \"2023-11-12T01:20:00.000Z\",\n",
    "    'pm1': 15.6,\n",
    "    'pm2_5': 26.400002,\n",
    "    'pm10': 27.3,\n",
    "    'temperature': 25.3,\n",
    "    'humidity': 70.8\n",
    "},{\n",
    "    'sensor_id': \"dc5475b0f97d\",\n",
    "    'timestamp': \"2023-11-12T02:35:00.000Z\",\n",
    "    'pm1': 10.65,\n",
    "    'pm2_5': 18.349998,\n",
    "    'pm10': 18.75,\n",
    "    'temperature': 24.6,\n",
    "    'humidity': 73.75\n",
    "}]\n",
    "\n",
    "# For testing purposes, see what documents will be used in RAG\n",
    "query_embedding = vector_store.embedding_function(query_1)\n",
    "relevant_documents= get_relevant_documents(query_embedding)\n",
    "\n",
    "execute(PROMPT, query_1, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0df0a928-31f3-4c5a-b3c3-b0437f24502e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "Expected: \n",
      " [{'sensor_id': 'AirBeam3-943cc67daabc', 'timestamp': '2023-10-06T21:55:17.000', 'pm1': 7.0, 'pm2_5': 8.5, 'pm10': 8.0, 'temperature': 90.0, 'humidity': 69.0}, {'sensor_id': 'AirBeam3-943cc67daabc', 'timestamp': '2023-10-06T15:13:20.000', 'pm1': 5.0, 'pm2_5': 7.0, 'pm10': 6.0, 'temperature': 100.0, 'humidity': 37.0}]\n",
      "\n",
      "Actual: \n",
      "  <thinking>\n",
      "To map the provided data to the AFRI_SET_COMMON format:\n",
      "\n",
      "1. The sensor_id field should be set to \"AirBeam3-943cc67daabc\" since that is the Sensor_Package_Name provided.\n",
      "\n",
      "2. The timestamp field should be taken from the Timestamp column.\n",
      "\n",
      "3. The pm0_3, pm0_5, pm1, pm2_5, pm4, pm5, and pm10 fields should be mapped from the 1:Measurement_Value, 2:Measurement_Value, 3:Measurement_Value, and 4:Measurement_Value columns based on matching the Measurement_Type and Measurement_Units values. \n",
      "\n",
      "For example, 2:Measurement_Value column has Measurement_Type as Particulate Matter and Measurement_Units as microgram per cubic meter, so it maps to the pm1 field.\n",
      "\n",
      "4. The temperature and humidity fields should be mapped from the 1:Measurement_Value and 5:Measurement_Value columns based on matching the Measurement_Type and Measurement_Units values.\n",
      "\n",
      "5. The latitude and longitude fields are not provided in this data, so they can be omitted or set to null.\n",
      "\n",
      "</thinking>\n",
      "\n",
      "<response>\n",
      "[\n",
      "  {\n",
      "    \"sensor_id\": \"AirBeam3-943cc67daabc\", \n",
      "    \"timestamp\": \"2023-10-06T21:55:17.000\",\n",
      "    \"pm0_3\": null,\n",
      "    \"pm0_5\": null,  \n",
      "    \"pm1\": 7.0,\n",
      "    \"pm2_5\": 8.5,\n",
      "    \"pm4\": null,\n",
      "    \"pm5\": null,\n",
      "    \"pm10\": 8.0,\n",
      "    \"temperature\": 90.0,\n",
      "    \"humidity\": 69.0\n",
      "  },\n",
      "  {\n",
      "    \"sensor_id\": \"AirBeam3-943cc67daabc\",\n",
      "    \"timestamp\": \"2023-10-06T15:13:20.000\", \n",
      "    \"pm0_3\": null,\n",
      "    \"pm0_5\": null,\n",
      "    \"pm1\": 5.0,\n",
      "    \"pm2_5\": 7.0,\n",
      "    \"pm4\": null,\n",
      "    \"pm5\": null,\n",
      "    \"pm10\": 6.0,\n",
      "    \"temperature\": 100.0,\n",
      "    \"humidity\": 37.0\n",
      "  }\n",
      "]\n",
      "</response>\n",
      "\n",
      "<confidence>90</confidence>\n"
     ]
    }
   ],
   "source": [
    "# This is an example from airbeam.csv\n",
    "\n",
    "query_2 = \"\"\"Map the following provided data to the AFRI_SET_COMMON formmat:\n",
    ",,,,,Sensor_Package_Name,Sensor_Package_Name,Sensor_Package_Name,Sensor_Package_Name,Sensor_Package_Name\n",
    ",,,,,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc\n",
    ",,,,,Sensor_Name,Sensor_Name,Sensor_Name,Sensor_Name,Sensor_Name\n",
    ",,,,,AirBeam3-F,AirBeam3-PM1,AirBeam3-PM10,AirBeam3-PM2.5,AirBeam3-RH\n",
    ",,,,,Measurement_Type,Measurement_Type,Measurement_Type,Measurement_Type,Measurement_Type\n",
    ",,,,,Temperature,Particulate Matter,Particulate Matter,Particulate Matter,Humidity\n",
    ",,,,,Measurement_Units,Measurement_Units,Measurement_Units,Measurement_Units,Measurement_Units\n",
    ",,,,,fahrenheit,microgram per cubic meter,microgram per cubic meter,microgram per cubic meter,percent\n",
    "ObjectID,Session_Name,Timestamp,Latitude,Longitude,1:Measurement_Value,2:Measurement_Value,3:Measurement_Value,4:Measurement_Value,5:Measurement_Value\n",
    "421,AfriSET (1),2023-10-06T21:55:17.000,5.65151,-0.185649,90.0,7.0,8.0,8.5,69.0\n",
    "20,AfriSET (1),2023-10-06T15:13:20.000,5.65151,-0.185649,100.0,5.0,6.0,7.0,37.0\n",
    "\"\"\"\n",
    "\n",
    "# this is just used for testing. The model never sees this\n",
    "expected = [{\n",
    "    'sensor_id': \"AirBeam3-943cc67daabc\",\n",
    "    'timestamp': \"2023-10-06T21:55:17.000\",\n",
    "    'pm1': 7.0,\n",
    "    'pm2_5': 8.5,\n",
    "    'pm10': 8.0,\n",
    "    'temperature': 90.0,\n",
    "    'humidity': 69.0\n",
    "}, {\n",
    "    'sensor_id': \"AirBeam3-943cc67daabc\",\n",
    "    'timestamp': \"2023-10-06T15:13:20.000\",\n",
    "    'pm1': 5.0,\n",
    "    'pm2_5': 7.0,\n",
    "    'pm10': 6.0,\n",
    "    'temperature': 100.0,\n",
    "    'humidity': 37.0\n",
    "}]\n",
    "\n",
    "\n",
    "# For testing purposes, see what documents will be used in RAG\n",
    "# The first step would be to create an embedding of the query such that it could be compared with the documents known to the vector store.\n",
    "relevant_documents = get_relevant_documents(query_embedding)\n",
    "query_embedding_= vector_store.embedding_function(query_2)\n",
    "\n",
    "execute(PROMPT, query_2, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea29c48c-f68b-40cb-8965-1c03eb87311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "Expected: \n",
      " [{'sensor_id': '3083988F1432', 'timestamp': '2023-11-29T05:20:00.000Z', 'pm0_3': 0, 'pm0_5': 59, 'pm1': 69, 'pm2_5': 69, 'pm4': 0, 'pm5': 0, 'pm10': 69, 'temperature': 27.7, 'humidity': 87.5}, {'sensor_id': '3083988F1432', 'timestamp': '2023-11-29T05:21:00.000Z', 'pm0_3': 0, 'pm0_5': 59, 'pm1': 68, 'pm2_5': 68, 'pm4': 0, 'pm5': 0, 'pm10': 68, 'temperature': 27.7, 'humidity': 87.9}, {'sensor_id': '3083988F1432', 'timestamp': '2023-11-29T05:22:00.000Z', 'pm0_3': 0, 'pm0_5': 64, 'pm1': 74, 'pm2_5': 74, 'pm4': 0, 'pm5': 0, 'pm10': 75, 'temperature': 27.7, 'humidity': 87.4}]\n",
      "\n",
      "Actual: \n",
      "  <thinking>\n",
      "To map the provided data to the AFRI_SET_COMMON format, I will:\n",
      "\n",
      "1. Map \"deviceid\" to \"sensor_id\"\n",
      "2. Map \"dt_time\" to \"timestamp\" \n",
      "3. Map the pollutant values (pm0.3cnt, pm0.5cnt, etc.) to the corresponding pm properties in the schema\n",
      "4. Map \"temp\" to \"temperature\"\n",
      "5. Map \"rh\" to \"humidity\"\n",
      "\n",
      "The location data (lat, lon) and other ancillary measurements are not required by the schema so I will omit them.\n",
      "</thinking>\n",
      "\n",
      "<response>\n",
      "{\n",
      "  \"confidence\": 90, \n",
      "  \"payload\": [\n",
      "    {\n",
      "      \"sensor_id\": \"3083988F1432\",\n",
      "      \"timestamp\": \"2023-11-29T05:20:00\",  \n",
      "      \"pm0_3\": 0,\n",
      "      \"pm0_5\": 59,\n",
      "      \"pm1\": 69,\n",
      "      \"pm2_5\": 9,\n",
      "      \"pm4\": 0, \n",
      "      \"pm5\": 69,\n",
      "      \"pm10\": 9,\n",
      "      \"temperature\": 27.7,\n",
      "      \"humidity\": 87.5\n",
      "    },\n",
      "    {\n",
      "      \"sensor_id\": \"3083988F1432\",\n",
      "      \"timestamp\": \"2023-11-29T05:21:00\",\n",
      "      \"pm0_3\": 0,\n",
      "      \"pm0_5\": 59,\n",
      "      \"pm1\": 68,\n",
      "      \"pm2_5\": 8,\n",
      "      \"pm4\": 0,\n",
      "      \"pm5\": 68,\n",
      "      \"pm10\": 8,\n",
      "      \"temperature\": 27.7,\n",
      "      \"humidity\": 87.9\n",
      "    },\n",
      "    {\n",
      "      \"sensor_id\": \"3083988F1432\",\n",
      "      \"timestamp\": \"2023-11-29T05:22:00\",\n",
      "      \"pm0_3\": 0,\n",
      "      \"pm0_5\": 64,\n",
      "      \"pm1\": 74,\n",
      "      \"pm2_5\": 9,\n",
      "      \"pm4\": 0,\n",
      "      \"pm5\": 75,\n",
      "      \"pm10\": 9,\n",
      "      \"temperature\": 27.7,\n",
      "      \"humidity\": 87.4\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "# This is an example from atmos.csv:\n",
    "\n",
    "query_3 = \"\"\"Map the following provided data to the AFRI_SET_COMMON formmat:\n",
    "pm4cnc,pm4cnt,dt_time,pm25raw,pm2.5cnc,temp,rh,o3op1,o3op2,no2op1,no2op2,pm10cnc,PM10,pres,altd,pm1cnc,pm0.3cnt,pm0.5cnt,pm1cnt,pm2.5cnt,pm5cnt,pm10cnt,lat,lon,no,nox,nh3,co,co2,benzene,deviceid\n",
    "0,0,2023-11-29 05:20:00,9,9,27.7,87.5,0,0,0,0,9,0,1002,308,8,0,59,69,69,0,69,0,0,0,0,0,0,0,0,3083988F1432\n",
    "0,0,2023-11-29 05:21:00,8,8,27.7,87.9,0,0,0,0,8,0,1001,309,8,0,59,68,68,0,68,0,0,0,0,0,0,0,0,3083988F1432\n",
    "0,0,2023-11-29 05:22:00,9,9,27.7,87.4,0,0,0,0,9,0,1001,308,8,0,64,74,74,0,75,0,0,0,0,0,0,0,0,3083988F1432\n",
    "\"\"\"\n",
    "\n",
    "# this is just used for testing. The model never sees this\n",
    "expected = [{\n",
    "    'sensor_id': \"3083988F1432\",\n",
    "    'timestamp': \"2023-11-29T05:20:00.000Z\",\n",
    "    'pm0_3': 0,\n",
    "    'pm0_5': 59,\n",
    "    'pm1': 69,\n",
    "    'pm2_5': 69,\n",
    "    'pm4': 0,\n",
    "    'pm5': 0,\n",
    "    'pm10': 69,\n",
    "    'temperature': 27.7,\n",
    "    'humidity': 87.5\n",
    "}, {\n",
    "    'sensor_id': \"3083988F1432\",\n",
    "    'timestamp': \"2023-11-29T05:21:00.000Z\",\n",
    "    'pm0_3': 0,\n",
    "    'pm0_5': 59,\n",
    "    'pm1': 68,\n",
    "    'pm2_5': 68,\n",
    "    'pm4': 0,\n",
    "    'pm5': 0,\n",
    "    'pm10': 68,\n",
    "    'temperature': 27.7,\n",
    "    'humidity': 87.9\n",
    "}, {\n",
    "    'sensor_id': \"3083988F1432\",\n",
    "    'timestamp': \"2023-11-29T05:22:00.000Z\",\n",
    "    'pm0_3': 0,\n",
    "    'pm0_5': 64,\n",
    "    'pm1': 74,\n",
    "    'pm2_5': 74,\n",
    "    'pm4': 0,\n",
    "    'pm5': 0,\n",
    "    'pm10': 75,\n",
    "    'temperature': 27.7,\n",
    "    'humidity': 87.4\n",
    "}]\n",
    "\n",
    "# For testing purposes, see what documents will be used in RAG\n",
    "query_embedding = vector_store.embedding_function(query_3)\n",
    "relevant_documents= get_relevant_documents(query_embedding)\n",
    "\n",
    "execute(PROMPT, query_3, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9355a-2aa1-4708-98d5-2f4a7f876572",
   "metadata": {},
   "source": [
    "# Ask for how to map, instead of just the result\n",
    "\n",
    "Create a prompt to generate something we can execute to carry out the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d17b0bad-a10c-49a8-a458-bb4c64b39220",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3420c89b-25f7-449b-b668-b8508e0cd974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "mapping_prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the context within the following <context></context> XML tag to provide a concise answer to the question at the end:\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "<code>\n",
    "from datetime import datetime\n",
    "from typing import TypeAlias\n",
    "import csv\n",
    "# TODO: add any other imports here\n",
    "\n",
    "@dataclass\n",
    "class MappedData:\n",
    "    sensor_id: str\n",
    "    timestamp: datetime \n",
    "    pm0_3: float\n",
    "    pm0_5: float\n",
    "    pm1: float  \n",
    "    pm2_5: float\n",
    "    pm4: float\n",
    "    pm5: float\n",
    "    pm10: float\n",
    "    temperature: float\n",
    "    humidity: float\n",
    "\n",
    "MappedDataList: TypeAlias = list(MappedData)\n",
    "\n",
    "class Converter:\n",
    "    \\\"\\\"\\\"A class that manages the conversion of a provided set of data, either in json or csv, to the AFRI_SET_COMMON format.\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    def convert(input:str) -> MappedDataList:\n",
    "        \\\"\\\"\\\"Converts the provided data to the AFRI_SET_COMMON format.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : str\n",
    "           Input data to be converted to the AFRI_SET_COMMON format.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        MappedDataList\n",
    "            A list of line items from the incoming data converted to the AFRI_SET_COMMON format.\n",
    "        \\\"\\\"\\\"\n",
    "        # TODO: add implementation here  \n",
    "</code>\n",
    "\n",
    "<response>\n",
    "    <python>\n",
    "        <!--insert generated python class here -->\n",
    "    </python>\n",
    "    <confidence>\n",
    "        <!--insert confidence rate of transform here -->\n",
    "    </confidence>\n",
    "    <test>\n",
    "        <!--insert executable python code here to test, which must pass -->\n",
    "    </test>\n",
    "</response>\n",
    "\n",
    "[Task instructions]\n",
    "You ALWAYS follow these guidelines when writing your response:\n",
    "<guidelines>\n",
    "- You will be acting as an expert software developer in Python writig code compliant with Python 3.10 that reads the input data provided as part of the question and transforms itto the AFRI_SET_COMMON json format. \n",
    "- The generated Python code should follow the structure as described in the <code></code> XML tags. \n",
    "- Use the python csv module to read the input data.\n",
    "- The input data will vary bewteen each invocation of the convert function, therefore nothing should be hardcoded within this function.\n",
    "- Ensure the code is syntactically correct, bug-free, optimized, not span multiple lines unnessarily, and prefer to use standard libraries. \n",
    "- The response must be limited to the structure as described in the <response></response> XML tags. Nothing else (i.e. context, steps, or explanation) should be returned as part of the response.\n",
    "</guidelines>\n",
    "\n",
    "When you reply, first determine how the provided input should be mapped to the AFRI_SET_COMMON json format. Write this mapping within the <thinking></thinking> XML tags. This is a space for you to write down relevant content and will not be shown to the user.  Once you are done extracting determing the mapping steps, answer the question.  Put your answer inside the <response></response> XML tags.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "MAPPING_PROMPT = PromptTemplate(\n",
    "    template=mapping_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b47bcb-9839-43f7-bcbf-c37348fe4d1a",
   "metadata": {},
   "source": [
    "Get the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e29716e-aa97-4d3f-9204-b5c0cefbca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <thinking>\n",
      "To map the provided CSV data to the AFRI_SET_COMMON format, I will:\n",
      "\n",
      "1. Import the necessary modules like csv, datetime, TypeAlias from typing\n",
      "\n",
      "2. Define the MappedData dataclass to match the AFRI_SET_COMMON format \n",
      "\n",
      "3. Implement the Converter class and convert() method to:\n",
      "   - Read the input CSV data using csv.reader\n",
      "   - Extract the relevant fields\n",
      "   - Construct MappedData objects \n",
      "   - Append them to a MappedDataList\n",
      "   - Return the MappedDataList\n",
      "\n",
      "The mapping logic will focus on:\n",
      "- sensor_id: Use the serialno field\n",
      "- timestamp: Parse the timestamp string to datetime \n",
      "- pm fields: Map pm01 to pm0_3, pm02 to pm0_5, pm10 stays as is  \n",
      "- temperature: Use atmp\n",
      "- humidity: Use rhum\n",
      "\n",
      "No data available for:\n",
      "- pm1\n",
      "- pm2_5\n",
      "- pm4 \n",
      "- pm5\n",
      "\n",
      "So I will set those to 0.\n",
      "</thinking>\n",
      "\n",
      "<response>\n",
      "    <python>\n",
      "@dataclass\n",
      "class MappedData:\n",
      "    sensor_id: str\n",
      "    timestamp: datetime \n",
      "    pm0_3: float\n",
      "    pm0_5: float \n",
      "    pm1: float  \n",
      "    pm2_5: float\n",
      "    pm4: float\n",
      "    pm5: float\n",
      "    pm10: float\n",
      "    temperature: float\n",
      "    humidity: float\n",
      "\n",
      "MappedDataList = list[MappedData]\n",
      "\n",
      "class Converter:\n",
      "\n",
      "    def convert(self, input: str) -> MappedDataList:\n",
      "        \n",
      "        data = []\n",
      "        \n",
      "        reader = csv.reader(input.splitlines())\n",
      "        next(reader) # skip header\n",
      "        \n",
      "        for row in reader:\n",
      "            sensor_id, _, pm01, pm02, pm10, _, atmp, rhum, _, _, _, timestamp, serialno, *_ = row\n",
      "            \n",
      "            timestamp = datetime.fromisoformat(timestamp)\n",
      "            \n",
      "            data.append(\n",
      "                MappedData(\n",
      "                    sensor_id=serialno,\n",
      "                    timestamp=timestamp, \n",
      "                    pm0_3=float(pm01),\n",
      "                    pm0_5=float(pm02),\n",
      "                    pm1=0,\n",
      "                    pm2_5=0,\n",
      "                    pm4=0,\n",
      "                    pm5=0,\n",
      "                    pm10=float(pm10),\n",
      "                    temperature=float(atmp),\n",
      "                    humidity=float(rhum)\n",
      "                )\n",
      "            )\n",
      "        \n",
      "        return data\n",
      "    </python>\n",
      "    <confidence>\n",
      "        100% \n",
      "    </confidence>\n",
      "    <test>\n",
      "converter = Converter()\n",
      "input = '''locationId,locationName,pm01,pm02,pm10,pm003Count,atmp,rhum,rco2,tvoc,wifi,timestamp,serialno,firmwareVersion,tvocIndex,noxIndex,datapoints\n",
      "59513,dc5475b0f97c,15.6,26.400002,27.3,2994.5,25.3,70.8,,,-69,2023-11-12T01:20:00.000Z,dc5475b0f97c,,,,2'''\n",
      "        \n",
      "result = converter.convert(input)\n",
      "        \n",
      "assert len(result) == 1\n",
      "assert result[0].sensor_id == 'dc5475b0f97c' \n",
      "assert result[0].pm0_3 == 15.6\n",
      "assert result[0].pm1 == 0\n",
      "    </test>\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "execute(MAPPING_PROMPT, query_1, '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "425914eb-4253-4503-88d1-4f27a6da1612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <thinking>\n",
      "To map the provided CSV data to the AFRI_SET_COMMON format, I need to:\n",
      "\n",
      "1. Map the column headers to the appropriate MappedData fields\n",
      "2. Extract the relevant data from each row into a MappedData object\n",
      "3. Append each MappedData object to a list\n",
      "4. Return the list of MappedData objects\n",
      "\n",
      "The mapping is:\n",
      "\n",
      "- ObjectID -> sensor_id \n",
      "- Timestamp -> timestamp\n",
      "- 1:Measurement_Value -> temperature\n",
      "- 2:Measurement_Value -> pm0_3\n",
      "- 3:Measurement_Value -> pm10 \n",
      "- 4:Measurement_Value -> pm2_5\n",
      "- 5:Measurement_Value -> humidity\n",
      "\n",
      "</thinking>\n",
      "\n",
      "<response>\n",
      "<python>\n",
      "from dataclasses import dataclass\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "import csv\n",
      "\n",
      "@dataclass\n",
      "class MappedData:\n",
      "    sensor_id: str\n",
      "    timestamp: datetime\n",
      "    pm0_3: float\n",
      "    pm0_5: float \n",
      "    pm1: float\n",
      "    pm2_5: float\n",
      "    pm4: float  \n",
      "    pm5: float\n",
      "    pm10: float\n",
      "    temperature: float\n",
      "    humidity: float\n",
      "\n",
      "def convert(input: str) -> List[MappedData]:\n",
      "    rows = csv.reader(input.splitlines())\n",
      "    next(rows) # skip header row\n",
      "    mapped_data_list = []\n",
      "    for row in rows:\n",
      "        mapped_data = MappedData(\n",
      "            sensor_id=row[0],\n",
      "            timestamp=datetime.fromisoformat(row[2]),\n",
      "            temperature=float(row[5]),\n",
      "            pm0_3=float(row[6]),\n",
      "            pm10=float(row[7]), \n",
      "            pm2_5=float(row[8]),\n",
      "            humidity=float(row[9]),\n",
      "            pm0_5=0.0,\n",
      "            pm1=0.0,\n",
      "            pm4=0.0,   \n",
      "            pm5=0.0\n",
      "        )\n",
      "        mapped_data_list.append(mapped_data)\n",
      "    return mapped_data_list\n",
      "</python>\n",
      "\n",
      "<confidence>100%</confidence>\n",
      "\n",
      "<test>\n",
      "input_data = \"\"\"\\\n",
      ",,,,,Sensor_Package_Name,Sensor_Package_Name,Sensor_Package_Name,Sensor_Package_Name,Sensor_Package_Name\n",
      ",,,,,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc,AirBeam3-943cc67daabc\n",
      ",,,,,Sensor_Name,Sensor_Name,Sensor_Name,Sensor_Name,Sensor_Name  \n",
      ",,,,,AirBeam3-F,AirBeam3-PM1,AirBeam3-PM10,AirBeam3-PM2.5,AirBeam3-RH\n",
      ",,,,,Measurement_Type,Measurement_Type,Measurement_Type,Measurement_Type,Measurement_Type\n",
      ",,,,,Temperature,Particulate Matter,Particulate Matter,Particulate Matter,Humidity  \n",
      ",,,,,Measurement_Units,Measurement_Units,Measurement_Units,Measurement_Units,Measurement_Units\n",
      ",,,,,fahrenheit,microgram per cubic meter,microgram per cubic meter,microgram per cubic meter,percent\n",
      "ObjectID,Session_Name,Timestamp,Latitude,Longitude,1:Measurement_Value,2:Measurement_Value,3:Measurement_Value,4:Measurement_Value,5:Measurement_Value\n",
      "421,AfriSET (1),2023-10-06T21:55:17.000,5.65151,-0.185649,90.0,7.0,8.0,8.5,69.0\n",
      "\"\"\"\n",
      "\n",
      "result = convert(input_data)\n",
      "assert len(result) == 1\n",
      "assert result[0].sensor_id == \"421\" \n",
      "assert result[0].timestamp == datetime(2023, 10, 6, 21, 55, 17)\n",
      "assert result[0].temperature == 90.0\n",
      "assert result[0].pm0_3 == 7.0\n",
      "assert result[0].pm10 == 8.0\n",
      "assert result[0].pm2_5 == 8.5 \n",
      "assert result[0].humidity == 69.0\n",
      "\n",
      "print(\"Test passed\")  \n",
      "</test>\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "execute(MAPPING_PROMPT, query_2, '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5740bde-02f0-4f49-b7c9-e0dc1b77e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <thinking>\n",
      "To map the provided data to the AFRI_SET_COMMON format, I need to:\n",
      "\n",
      "1. Define a MappedData dataclass with the appropriate fields\n",
      "2. Implement the Converter.convert() method to:\n",
      "   - Read the input data (CSV format)\n",
      "   - Map each row to a MappedData instance:\n",
      "     - sensor_id: deviceid\n",
      "     - timestamp: convert dt_time to datetime object  \n",
      "     - pm0_3: pm0.3cnt\n",
      "     - pm0_5: pm0.5cnt \n",
      "     - ...\n",
      "     - temperature: temp\n",
      "     - humidity: rh\n",
      "   - Append each MappedData instance to a MappedDataList\n",
      "   - Return the MappedDataList\n",
      "</thinking>\n",
      "\n",
      "<response>\n",
      "    <python>\n",
      "@dataclass\n",
      "class MappedData:\n",
      "    sensor_id: str\n",
      "    timestamp: datetime\n",
      "    pm0_3: float\n",
      "    pm0_5: float\n",
      "    pm1: float  \n",
      "    pm2_5: float\n",
      "    pm4: float\n",
      "    pm5: float\n",
      "    pm10: float\n",
      "    temperature: float\n",
      "    humidity: float\n",
      "\n",
      "MappedDataList = List[MappedData]\n",
      "\n",
      "class Converter:\n",
      "    \n",
      "    def convert(input: str) -> MappedDataList:\n",
      "        data = []\n",
      "        reader = csv.reader(input.splitlines())\n",
      "        next(reader) # skip header\n",
      "        \n",
      "        for row in reader:\n",
      "            deviceid, _, dt_time, pm25raw, pm2_5cnc, temp, rh, *_ = row            \n",
      "            data.append(\n",
      "                MappedData(\n",
      "                    sensor_id=deviceid,\n",
      "                    timestamp=datetime.strptime(dt_time, \"%Y-%m-%d %H:%M:%S\"),  \n",
      "                    pm0_3=float(row[16]),\n",
      "                    pm0_5=float(row[17]),\n",
      "                    pm1=float(row[18]),\n",
      "                    pm2_5=float(row[20]),\n",
      "                    pm4=0.0,\n",
      "                    pm5=0.0,    \n",
      "                    pm10=float(row[21]),\n",
      "                    temperature=float(temp),\n",
      "                    humidity=float(rh)\n",
      "                )\n",
      "            )\n",
      "        \n",
      "        return data\n",
      "    </python>\n",
      "    <confidence>\n",
      "        95%\n",
      "    </confidence>\n",
      "    <test>\n",
      "input_data = \"\"\"\\\n",
      "pm4cnc,pm4cnt,dt_time,pm25raw,pm2.5cnc,temp,rh,o3op1,o3op2,no2op1,no2op2,pm10cnc,PM10,pres,altd,pm1cnc,pm0.3cnt,pm0.5cnt,pm1cnt,pm2.5cnt,pm5cnt,pm10cnt,lat,lon,no,nox,nh3,co,co2,benzene,deviceid\n",
      "0,0,2023-11-29 05:20:00,9,9,27.7,87.5,0,0,0,0,9,0,1002,308,8,0,59,69,69,0,69,0,0,0,0,0,0,0,0,3083988F1432\n",
      "0,0,2023-11-29 05:21:00,8,8,27.7,87.9,0,0,0,0,8,0,1001,309,8,0,59,68,68,0,68,0,0,0,0,0,0,0,0,3083988F1432  \n",
      "\"\"\"\n",
      "\n",
      "converter = Converter() \n",
      "result = converter.convert(input_data)\n",
      "\n",
      "assert len(result) == 2\n",
      "assert result[0].sensor_id == \"3083988F1432\"\n",
      "assert result[0].timestamp == datetime(2023, 11, 29, 5, 20)\n",
      "assert result[0].pm2_5 == 9  \n",
      "    </test>\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "execute(MAPPING_PROMPT, query_3, '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cded47-43f4-4ea8-9ee7-4d449b2ba471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
