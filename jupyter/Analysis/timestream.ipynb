{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495cd0205ff6ae13",
   "metadata": {},
   "source": [
    "# Using AWS Timestream to query sensor feed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ef8b600c28934",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This Jupyter notebook guides you through the process of querying sensor data from AWS Timestream. Timestream is AWS's cutting-edge serverless database, purpose-built for efficiently storing time-series data, especially data originating from IoT devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d9781ba10563c0",
   "metadata": {},
   "source": [
    "## Why Timestream\n",
    "\n",
    "Amazon Timestream is a purpose-built, fully managed time-series database service designed to simplify the storage and analysis of time-stamped data at scale. Here are several compelling reasons to choose Amazon Timestream:\n",
    "\n",
    "1. **Time-Series Data Management:**\n",
    "   - *Efficient Storage:* Timestream is optimized for handling time-series data, providing high efficiency in storage and query performance for timestamped information.\n",
    "\n",
    "2. **Serverless and Fully Managed:**\n",
    "   - *Ease of Use:* Timestream is fully managed, eliminating operational overhead. It is serverless, automatically handling tasks such as scaling, patching, and backups.\n",
    "\n",
    "3. **Scalability:**\n",
    "   - *Horizontal Scalability:* Timestream scales horizontally, accommodating a large volume of time-series data as your application grows.\n",
    "\n",
    "4. **Cost-Effective:**\n",
    "   - *Pay-as-You-Go Model:* With a pay-as-you-go pricing model, Timestream is cost-effective, especially for varying workloads.\n",
    "\n",
    "5. **Built-in Analytics:**\n",
    "   - *Query Language:* Timestream provides a SQL-like query language for easy data retrieval, supporting analytical queries for complex analysis on time-series data.\n",
    "\n",
    "6. **Integration with AWS Ecosystem:**\n",
    "   - *Seamless Integration:* Timestream integrates seamlessly with AWS services like Amazon CloudWatch, AWS IoT, and Amazon Kinesis, allowing the ingestion and analysis of data from various sources.\n",
    "\n",
    "7. **Security and Compliance:**\n",
    "   - *AWS Identity and Access Management (IAM):* Timestream integrates with IAM for access control, ensuring secure data handling. It also supports encryption at rest and in transit to meet compliance requirements.\n",
    "\n",
    "8. **Versatility:**\n",
    "   - *Multi-Resolution Storage:* Timestream supports multi-resolution storage, enabling optimization of storage costs based on data access patterns.\n",
    "\n",
    "9. **Time-Windowed Retention Policies:**\n",
    "   - *Data Retention Control:* Timestream allows the definition of retention policies based on time windows, automatically managing the lifecycle of time-series data.\n",
    "\n",
    "10. **Real-time Data Ingestion:**\n",
    "    - *High Throughput:* Timestream supports high-speed, continuous data ingestion, making it suitable for real-time applications and scenarios with rapidly generated data.\n",
    "\n",
    "In summary, Amazon Timestream is an excellent choice for applications requiring efficient storage, analysis, and retrieval of time-series data at scale. It is particularly well-suited for use cases such as IoT, telemetry, monitoring, and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e92f7318723977",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd7e314b64af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa639980ceee0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367011301978eda0",
   "metadata": {},
   "source": [
    "## Setup AWS Credentials in Jupyter Notebook\n",
    "\n",
    "To set up boto3 credentials in a Jupyter notebook:# Setting up Boto3 Credentials in a Jupyter Notebook\n",
    "\n",
    "To establish Boto3 credentials in a Jupyter notebook, follow these steps:\n",
    "\n",
    "1. **Configure AWS Credentials on the EC2 Instance:**\n",
    "   - Create a credentials file at `~/.aws/credentials`.\n",
    "   - Add the `aws_access_key_id` and `aws_secret_access_key` to this file.\n",
    "\n",
    "     ```plaintext\n",
    "     [default]\n",
    "     aws_access_key_id = YOUR_ACCESS_KEY_ID\n",
    "     aws_secret_access_key = YOUR_SECRET_ACCESS_KEY\n",
    "     ```\n",
    "\n",
    "2. **Alternative Configuration using Environment Variables:**\n",
    "   - Set environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` with your credentials.\n",
    "\n",
    "These steps will enable you to set up and use Boto3 credentials within your Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b3ad000e680bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "\n",
    "session = boto3.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d838b81b8a2bd",
   "metadata": {},
   "source": [
    "## Set environment and region variable to connect to the right database \n",
    "\n",
    "Replace the `ENVIRONMENT` variable with the specific environment used during deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09ab8c6991440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT='dev' # Replace this with the environment that you used when deploying the solution \n",
    "\n",
    "# Insert your database name here\n",
    "DATABASE_NAME=f'afriset-{ENVIRONMENT}'\n",
    "TABLE_NAME=f'afriset-{ENVIRONMENT}-sensor-feeds'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e581355dab5fdc",
   "metadata": {},
   "source": [
    "## Query Helper Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d36b24b9a458cc",
   "metadata": {},
   "source": [
    "Upon completing the file upload through the user interface (UI), the sensor data has been successfully ingested into Timestream. To retrieve this data, we will proceed by creating a Query object capable of executing any written query. The output will be iterated through and displayed on the screen. You can find the relevant code on GitHub at https://github.com/awslabs/amazon-timestream-tools/blob/master/sample_apps/python/QueryExample.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2ca2ee291f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query(object):\n",
    "\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.paginator = client.get_paginator('query')\n",
    "\n",
    "    # See records ingested into this table so far\n",
    "    SELECT_ALL = f\"SELECT * FROM {DATABASE_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "    def run_query(self, query_string):\n",
    "        try:\n",
    "            page_iterator = self.paginator.paginate(QueryString=query_string)\n",
    "            for page in page_iterator:\n",
    "                self._parse_query_result(page)\n",
    "        except Exception as err:\n",
    "            print(\"Exception while running query:\", err)\n",
    "\n",
    "    def _parse_query_result(self, query_result):\n",
    "        column_info = query_result['ColumnInfo']\n",
    "\n",
    "        print(\"Metadata: %s\" % column_info)\n",
    "        print(\"Data: \")\n",
    "        for row in query_result['Rows']:\n",
    "            print(self._parse_row(column_info, row))\n",
    "\n",
    "    def _parse_row(self, column_info, row):\n",
    "        data = row['Data']\n",
    "        row_output = []\n",
    "        for j in range(len(data)):\n",
    "            info = column_info[j]\n",
    "            datum = data[j]\n",
    "            row_output.append(self._parse_datum(info, datum))\n",
    "\n",
    "        return \"{%s}\" % str(row_output)\n",
    "\n",
    "    def _parse_datum(self, info, datum):\n",
    "        if datum.get('NullValue', False):\n",
    "            return \"%s=NULL\" % info['Name'],\n",
    "\n",
    "        column_type = info['Type']\n",
    "\n",
    "        # If the column is of TimeSeries Type\n",
    "        if 'TimeSeriesMeasureValueColumnInfo' in column_type:\n",
    "            return self._parse_time_series(info, datum)\n",
    "\n",
    "        # If the column is of Array Type\n",
    "        elif 'ArrayColumnInfo' in column_type:\n",
    "            array_values = datum['ArrayValue']\n",
    "            return \"%s=%s\" % (info['Name'], self._parse_array(info['Type']['ArrayColumnInfo'], array_values))\n",
    "\n",
    "        # If the column is of Row Type\n",
    "        elif 'RowColumnInfo' in column_type:\n",
    "            row_column_info = info['Type']['RowColumnInfo']\n",
    "            row_values = datum['RowValue']\n",
    "            return self._parse_row(row_column_info, row_values)\n",
    "\n",
    "        # If the column is of Scalar Type\n",
    "        else:\n",
    "            return self._parse_column_name(info) + datum['ScalarValue']\n",
    "\n",
    "    def _parse_time_series(self, info, datum):\n",
    "        time_series_output = []\n",
    "        for data_point in datum['TimeSeriesValue']:\n",
    "            time_series_output.append(\"{time=%s, value=%s}\"\n",
    "                                      % (data_point['Time'],\n",
    "                                         self._parse_datum(info['Type']['TimeSeriesMeasureValueColumnInfo'],\n",
    "                                                           data_point['Value'])))\n",
    "        return \"[%s]\" % str(time_series_output)\n",
    "\n",
    "    def _parse_array(self, array_column_info, array_values):\n",
    "        array_output = []\n",
    "        for datum in array_values:\n",
    "            array_output.append(self._parse_datum(array_column_info, datum))\n",
    "\n",
    "        return \"[%s]\" % str(array_output)\n",
    "\n",
    "    def run_query_with_multiple_pages(self, limit):\n",
    "        query_with_limit = self.SELECT_ALL + \" LIMIT \" + str(limit)\n",
    "        print(\"Starting query with multiple pages : \" + query_with_limit)\n",
    "        self.run_query(query_with_limit)\n",
    "\n",
    "    def cancel_query(self):\n",
    "        print(\"Starting query: \" + self.SELECT_ALL)\n",
    "        result = self.client.query(QueryString=self.SELECT_ALL)\n",
    "        print(\"Cancelling query: \" + self.SELECT_ALL)\n",
    "        try:\n",
    "            self.client.cancel_query(QueryId=result['QueryId'])\n",
    "            print(\"Query has been successfully cancelled\")\n",
    "        except Exception as err:\n",
    "            print(\"Cancelling query failed:\", err)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_column_name(info):\n",
    "        if 'Name' in info:\n",
    "            return info['Name'] + \"=\"\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4954a192b9749f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "query_client = session.client('timestream-query', region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n",
    "\n",
    "query = Query(query_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26c1b04c0c04c4",
   "metadata": {},
   "source": [
    "## Querying the Sensor Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48353f4af803812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = f\"\"\"\n",
    "        SELECT * FROM \"{DATABASE_NAME}\".\"{TABLE_NAME}\" WHERE time between ago(15m) and now() ORDER BY time DESC LIMIT 10 \n",
    "        \"\"\"\n",
    "\n",
    "query_output = query.run_query(QUERY_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting Data from Amazon Timestream to Amazon S3 for Further Analysis\n",
    "\n",
    "To export data from Amazon Timestream to Amazon S3, follow these steps:\n",
    "\n",
    "1. **Set Up IAM Roles:**\n",
    "   - Ensure that you have the required IAM roles with permissions allowing Timestream to write data to S3. Include at least the `timestream:Select` and `s3:PutObject` permissions.\n",
    "\n",
    "2. **Define an S3 Bucket:**\n",
    "   - Create an S3 bucket that will serve as the destination for the exported data.\n",
    "\n",
    "3. **Run a Timestream Query:**\n",
    "   - Utilize the Timestream Query language to retrieve the desired data. For example:\n",
    "\n",
    "     ```sql\n",
    "     SELECT * FROM \"your_database\".\"your_table\" WHERE time >= ago(7d)\n",
    "     ```\n",
    "\n",
    "4. **Execute the Query with INTO S3 Syntax:**\n",
    "   - Modify the Timestream query to include the `INTO S3` syntax, specifying the S3 bucket and key for the data export. For example:\n",
    "\n",
    "     ```sql\n",
    "     SELECT * FROM \"your_database\".\"your_table\" WHERE time >= ago(7d)\n",
    "     INTO S3 's3://your-export-bucket/export-path/'\n",
    "     ```\n",
    "\n",
    "   Ensure you replace `\"your_database\"`, `\"your_table\"`, and `'s3://your-export-bucket/export-path/'` with your actual Timestream database, table, and S3 bucket details.\n",
    "\n",
    "5. **Execute the Query:**\n",
    "   - Run the modified query to initiate the export process from Timestream to S3.\n",
    "\n",
    "6. **Monitor the Export Process:**\n",
    "   - Keep track of the export process and check the S3 bucket for the exported files.\n",
    "\n",
    "Here's an example Python code snippet using the `boto3` library to execute a Timestream query with the `INTO S3` syntax:\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "timestream = boto3.client('timestream-query')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM \"your_database\".\"your_table\" WHERE time >= ago(7d)\n",
    "INTO S3 's3://your-export-bucket/export-path/'\n",
    "\"\"\"\n",
    "\n",
    "response = timestream.query(QueryString=query)\n",
    "```\n",
    "\n",
    "Make sure to replace \"your_database\", \"your_table\", and 's3://your-export-bucket/export-path/' with your specific Timestream database, table, and S3 bucket details.\n",
    "\n",
    "Ensure secure handling of credentials and confirm that IAM roles have the necessary permissions for both Timestream and S3 operations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "609b7180768b10c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
